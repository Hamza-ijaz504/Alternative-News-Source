{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d75bd9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download 'punkt' to C:\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download 'punkt_tab' to C:\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download 'stopwords' to C:\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download attempts finished.\n",
      "Successfully imported word_tokenize.\n",
      "word_tokenize test successful: ['This', 'is', 'a', 'test', 'sentence', 'for', 'tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "# New Cell for focused download (or modify your existing focused download cell)\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "custom_nltk_data_path = \"C:\\\\nltk_data\"\n",
    "if not os.path.exists(custom_nltk_data_path):\n",
    "    try:\n",
    "        os.makedirs(custom_nltk_data_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create folder {custom_nltk_data_path}: {e}\")\n",
    "\n",
    "if custom_nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(custom_nltk_data_path)\n",
    "\n",
    "print(f\"Attempting to download 'punkt' to {custom_nltk_data_path}...\")\n",
    "nltk.download('punkt', download_dir=custom_nltk_data_path, quiet=False, force=True)\n",
    "\n",
    "\n",
    "print(f\"Attempting to download 'punkt_tab' to {custom_nltk_data_path}...\")\n",
    "nltk.download('punkt_tab', download_dir=custom_nltk_data_path, quiet=False, force=True)\n",
    "\n",
    "\n",
    "print(f\"Attempting to download 'stopwords' to {custom_nltk_data_path}...\")\n",
    "nltk.download('stopwords', download_dir=custom_nltk_data_path, quiet=False, force=True)\n",
    "\n",
    "print(\"Download attempts finished.\")\n",
    "\n",
    "# Quick test\n",
    "try:\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    print(\"Successfully imported word_tokenize.\")\n",
    "    test_tokens = word_tokenize(\"This is a test sentence for tokenization.\")\n",
    "    print(\"word_tokenize test successful:\", test_tokens)\n",
    "except Exception as e:\n",
    "    print(\"Error during quick test of word_tokenize:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a521a288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\nltk_data is already in NLTK data path.\n",
      "NLTK stopwords found.\n",
      "NLTK punkt tokenizer found.\n",
      "--- Testing Article Fetching and NLTK Preprocessing ---\n",
      "Attempting to fetch article from: https://www.bbc.com/news/articles/c87j5v4xjxqo\n",
      "Successfully fetched: Sidhu Moose Wala: Gangster tells BBC why India's biggest hip-hop star was murdered\n",
      "\n",
      "Title: Sidhu Moose Wala: Gangster tells BBC why India's biggest hip-hop star was murdered\n",
      "\n",
      "--- Raw Content (first 500 chars) ---\n",
      "Gangster tells BBC why India's biggest hip-hop star was murdered\n",
      "\n",
      "11 June 2025 Share Save Soutik Biswas & Ishleen Kaur BBC Eye Investigations Share Save\n",
      "\n",
      "BBC Sidhu Moose Wala was shot dead in a hail of bullets in 2022\n",
      "\n",
      "It was a killing that shocked India: Punjabi hip-hop star Sidhu Moose Wala shot dead through the windscreen of his car by hired gunmen. Within hours, a Punjabi gangster named Goldy Brar had used Facebook to claim responsibility for ordering the hit. But three years after the murde...\n",
      "\n",
      "--- NLTK Processed Content (first 500 chars) ---\n",
      "gangster tells bbc india biggest hip hop star murdered june share save soutik biswas ishleen kaur bbc eye investigations share save bbc sidhu moose wala shot dead hail bullets killing shocked india punjabi hip hop star sidhu moose wala shot dead windscreen car hired gunmen within hours punjabi gangster named goldy brar used facebook claim responsibility ordering hit three years murder one faced trial goldy brar still run whereabouts unknown bbc eye managed make contact brar challenged sidhu moos...\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Article Fetching/Preprocessing Setup\n",
    "\n",
    "import nltk\n",
    "import os \n",
    "from newspaper import Article \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# --- Point NLTK to a specific data directory ---\n",
    "\n",
    "custom_nltk_data_path = \"C:\\\\nltk_data\" # Use double backslashes for Windows paths\n",
    "\n",
    "if not os.path.exists(custom_nltk_data_path):\n",
    "    try:\n",
    "        os.makedirs(custom_nltk_data_path)\n",
    "        print(f\"Created NLTK data directory: {custom_nltk_data_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create NLTK data directory {custom_nltk_data_path}. Please create it manually. Error: {e}\")\n",
    "\n",
    "if custom_nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(custom_nltk_data_path)\n",
    "    print(f\"Added {custom_nltk_data_path} to NLTK data path.\")\n",
    "else:\n",
    "    print(f\"{custom_nltk_data_path} is already in NLTK data path.\")\n",
    "# --- End NLTK data path setup ---\n",
    "\n",
    "\n",
    "# Download NLTK resources if not already present, TO THE SPECIFIED DIRECTORY\n",
    "try:\n",
    "    # Check if resources are found in ANY of the NLTK paths\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    print(\"NLTK stopwords found.\")\n",
    "except LookupError: # More specific exception\n",
    "    print(f\"Downloading NLTK stopwords to {custom_nltk_data_path}...\")\n",
    "    nltk.download('stopwords', download_dir=custom_nltk_data_path, quiet=False) # quiet=False for visibility\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"NLTK punkt tokenizer found.\")\n",
    "except LookupError:\n",
    "    print(f\"Downloading NLTK punkt tokenizer to {custom_nltk_data_path}...\")\n",
    "    nltk.download('punkt', download_dir=custom_nltk_data_path, quiet=False) # quiet=False for visibility\n",
    "\n",
    "\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "\n",
    "def fetch_and_preprocess_article_nltk(url):\n",
    "    \"\"\"Fetches article from URL and preprocesses its text using NLTK.\"\"\"\n",
    "    print(f\"Attempting to fetch article from: {url}\")\n",
    "    try:\n",
    "        article_obj = Article(url)\n",
    "        article_obj.download()\n",
    "        article_obj.parse()\n",
    "        raw_text = article_obj.text\n",
    "        title = article_obj.title\n",
    "        # Check if title is None or empty, which can happen if parsing fails\n",
    "        if not title:\n",
    "            title = \"Title not found\"\n",
    "            print(\"Warning: Article title not found by newspaper3k.\")\n",
    "        else:\n",
    "            print(f\"Successfully fetched: {title}\")\n",
    "\n",
    "        # Check if raw_text is None or very short, indicating a fetching/parsing issue\n",
    "        if not raw_text or len(raw_text) < 100: # Arbitrary short length check\n",
    "             print(f\"Warning: Fetched raw text is very short or None for {url}. Content might be missing or paywalled/JS-rendered.\")\n",
    "             # return None, title, None # Optionally return early if text is bad\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching/parsing article from {url}: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "    if not raw_text: # Double check after the try-except\n",
    "        print(f\"No content found for article (raw_text is None): {url}\")\n",
    "        return None, title, None # title might have been set if initial parse got that far\n",
    "\n",
    "    # NLTK Preprocessing\n",
    "    text_lower = raw_text.lower()\n",
    "    text_no_punct = re.sub(r'\\W', ' ', text_lower) # Remove non-alphanumeric as punctuation\n",
    "    text_no_extra_space = re.sub(r'\\s+', ' ', text_no_punct).strip() # Remove extra whitespace\n",
    "    \n",
    "    tokens = word_tokenize(text_no_extra_space) # This is where the LookupError was happening\n",
    "    \n",
    "    filtered_tokens = [\n",
    "        word for word in tokens\n",
    "        if word not in stop_words_nltk and len(word) > 2 and word.isalpha() # Keep only alpha words > 2 chars\n",
    "    ]\n",
    "    preprocessed_text = \" \".join(filtered_tokens)\n",
    "\n",
    "    return raw_text, title, preprocessed_text\n",
    "\n",
    "# --- Test the function ---\n",
    "test_article_url = 'https://www.bbc.com/news/articles/c87j5v4xjxqo'\n",
    "\n",
    "print(\"--- Testing Article Fetching and NLTK Preprocessing ---\")\n",
    "raw_content, article_title, processed_content_nltk = fetch_and_preprocess_article_nltk(test_article_url)\n",
    "\n",
    "if raw_content:\n",
    "    print(f\"\\nTitle: {article_title}\") # article_title should be defined even if raw_content is minimal\n",
    "    print(\"\\n--- Raw Content (first 500 chars) ---\")\n",
    "    print(raw_content[:500] + \"...\")\n",
    "    if processed_content_nltk:\n",
    "        print(\"\\n--- NLTK Processed Content (first 500 chars) ---\")\n",
    "        print(processed_content_nltk[:500] + \"...\")\n",
    "    else:\n",
    "        print(\"\\n--- NLTK Processed Content: Not generated (likely due to issues with raw_content or tokenization).\")\n",
    "\n",
    "\n",
    "    documents_for_bertopic = [raw_content] if raw_content else []\n",
    "    processed_documents_for_bertopic = [processed_content_nltk] if processed_content_nltk else []\n",
    "else:\n",
    "    print(f\"\\nCould not get raw content for article: {test_article_url}\")\n",
    "    # Initialize these as empty lists if raw_content is None to avoid errors later\n",
    "    documents_for_bertopic = []\n",
    "    processed_documents_for_bertopic = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f08db64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hamza\\OneDrive\\Desktop\\Alternative News Source\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-06-28 23:12:12,158 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Topic Extraction on a Single Document by Splitting into Sentences ---\n",
      "Splitting the article into sentences...\n",
      "Successfully split the article into 79 sentences.\n",
      "Using 79 sentences after filtering short ones.\n",
      "\n",
      "Fitting BERTopic model on the list of sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:02<00:00,  1.50it/s]\n",
      "2025-06-28 23:12:24,271 - BERTopic - Embedding - Completed ✓\n",
      "2025-06-28 23:12:24,272 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-06-28 23:12:38,433 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-06-28 23:12:38,433 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-06-28 23:12:38,461 - BERTopic - Cluster - Completed ✓\n",
      "2025-06-28 23:12:38,462 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-06-28 23:12:38,503 - BERTopic - Representation - Completed ✓\n",
      "2025-06-28 23:12:38,503 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-06-28 23:12:38,518 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-06-28 23:12:38,548 - BERTopic - Representation - Completed ✓\n",
      "2025-06-28 23:12:38,550 - BERTopic - Topic reduction - Reduced number of topics from 6 to 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BERTopic Results ---\n",
      "Topic Info:\n",
      "   Topic  Count                      Name  \\\n",
      "0     -1     16        -1_the_and_in_with   \n",
      "1      0     31      0_wala_moose_and_the   \n",
      "2      1     13           1_he_was_it_for   \n",
      "3      2      8           2_the_of_in_had   \n",
      "4      3      7  3_bishnoi_student_is_the   \n",
      "5      4      4      4_get_as_simple_same   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [the, and, in, with, sport, need, village, gan...   \n",
      "1  [wala, moose, and, the, to, in, of, with, sidh...   \n",
      "2  [he, was, it, for, but, or, people, money, him...   \n",
      "3  [the, of, in, had, was, his, bbc, punjabi, ind...   \n",
      "4  [bishnoi, student, is, the, brar, and, in, law...   \n",
      "5  [get, as, simple, same, water, want, someone, ...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [This led to a spell in jail which hardened hi...  \n",
      "1  [\"Lawrence [Bishnoi] was in touch with Sidhu [...  \n",
      "2  [\"The day he died, people cried for him., So, ...  \n",
      "3  [In the weeks that followed the murder, about ...  \n",
      "4  [Bishnoi, once a student leader steeped in Pun...  \n",
      "5  [As simple as that.\", \"Get some water.\", \"Some...  \n",
      "\n",
      "Keywords for each topic:\n",
      "Topic 0: [('wala', 0.08670322406681358), ('moose', 0.08670322406681358), ('and', 0.0632962400617238), ('the', 0.062350360103874465), ('to', 0.055825054657017335), ('in', 0.05458823850485202), ('of', 0.046639334782322794), ('with', 0.04454920711933487), ('sidhu', 0.0374857292612195), ('brar', 0.0348943921831722)]\n",
      "Topic 1: [('he', 0.1723858831223018), ('was', 0.15177898079898372), ('it', 0.10952079675793563), ('for', 0.10582151521257306), ('but', 0.10582151521257306), ('or', 0.10257606241945856), ('people', 0.09385843725078935), ('money', 0.09385843725078935), ('him', 0.08803310711190304), ('his', 0.0861929415611509)]\n",
      "Topic 2: [('the', 0.09215687371450713), ('of', 0.07878309931174249), ('in', 0.0664457124067668), ('had', 0.05795930664526246), ('was', 0.05676287086791262), ('his', 0.05157561381220087), ('bbc', 0.051041149090228194), ('punjabi', 0.051041149090228194), ('india', 0.047490631217349866), ('save', 0.04603413532970824)]\n",
      "Topic 3: [('bishnoi', 0.09947715836089818), ('student', 0.08675786318607302), ('is', 0.07822765934895454), ('the', 0.07617806093336275), ('brar', 0.07105511311492727), ('and', 0.06512312644720648), ('in', 0.06277129513081656), ('lawrence', 0.056255029911273007), ('one', 0.056255029911273007), ('india', 0.05234182472879958)]\n",
      "Topic 4: [('get', 0.4746165456649877), ('as', 0.38178742743359695), ('simple', 0.31806892773294815), ('same', 0.31806892773294815), ('water', 0.31806892773294815), ('want', 0.27755875713500555), ('someone', 0.27755875713500555), ('some', 0.27755875713500555), ('out', 0.25396988903154766), ('car', 0.25396988903154766)]\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Topic Extraction with BERTopic (Final Attempt - with Sentence Splitting)\n",
    "\n",
    "from bertopic import BERTopic\n",
    "import hdbscan\n",
    "from nltk.tokenize import sent_tokenize # Import the sentence tokenizer from NLTK\n",
    "\n",
    "print(\"\\n--- Topic Extraction on a Single Document by Splitting into Sentences ---\")\n",
    "\n",
    "# We will use the 'raw_content' variable from the first cell\n",
    "if 'raw_content' in locals() and raw_content and len(raw_content) > 1:\n",
    "\n",
    "    # 1. Split the single document (raw_content) into a list of sentences\n",
    "    print(\"Splitting the article into sentences...\")\n",
    "    try:\n",
    "        # NLTK's sentence tokenizer is generally reliable\n",
    "        sentences = sent_tokenize(raw_content)\n",
    "        print(f\"Successfully split the article into {len(sentences)} sentences.\")\n",
    "        \n",
    "        # Optional: Filter out very short sentences if they might be noise (e.g., just a date or byline)\n",
    "        min_sentence_length = 10 # in characters\n",
    "        sentences = [s for s in sentences if len(s) > min_sentence_length]\n",
    "        print(f\"Using {len(sentences)} sentences after filtering short ones.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during sentence tokenization: {e}\")\n",
    "        sentences = [] # Set to empty list to prevent further errors\n",
    "\n",
    "    # Proceed only if we have enough sentences to work with\n",
    "    if len(sentences) > 5: # Arbitrary threshold, clustering needs a decent number of points\n",
    "        \n",
    "        # 2. Initialize BERTopic Model\n",
    "        # We can go back to using UMAP now, as it will have enough data points (sentences) to work with.\n",
    "        # Or stick with PCA, which is often faster. Let's try UMAP again.\n",
    "        topic_model = BERTopic(\n",
    "            embedding_model=\"all-MiniLM-L6-v2\",\n",
    "            # We don't need to specify umap_model or hdbscan_model if defaults are okay.\n",
    "            # Let's use the defaults first.\n",
    "            min_topic_size=3, # A topic must contain at least 3 sentences\n",
    "            nr_topics=\"auto\",\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        # 3. Fit the model on the list of sentences\n",
    "        print(\"\\nFitting BERTopic model on the list of sentences...\")\n",
    "        try:\n",
    "            topics, probabilities = topic_model.fit_transform(sentences)\n",
    "\n",
    "            print(\"\\n--- BERTopic Results ---\")\n",
    "            topic_info_df = topic_model.get_topic_info()\n",
    "            print(\"Topic Info:\")\n",
    "            print(topic_info_df)\n",
    "\n",
    "            if not topic_info_df.empty and not (len(topic_info_df) == 1 and topic_info_df.iloc[0][\"Topic\"] == -1):\n",
    "                print(\"\\nKeywords for each topic:\")\n",
    "                for topic_id in topic_info_df[\"Topic\"]:\n",
    "                    if topic_id != -1:\n",
    "                        topic_keywords = topic_model.get_topic(topic_id)\n",
    "                        print(f\"Topic {topic_id}: {topic_keywords}\")\n",
    "            else:\n",
    "                print(\"\\nNo distinct topics found. All sentences were considered outliers or the result was empty.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nAn error occurred during BERTopic fit_transform: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nNot enough sentences in the article to perform topic modeling. Please try a longer article.\")\n",
    "\n",
    "else:\n",
    "    print(\"Variable 'raw_content' not found or is empty. Please run the previous cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa50cdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Performing Sentiment Analysis on the Article ---\n",
      "Downloading VADER lexicon...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing sentiment of the entire article...\n",
      "\n",
      "Overall Article Sentiment Scores:\n",
      "  - Negative: 0.137\n",
      "  - Neutral:  0.797\n",
      "  - Positive: 0.066\n",
      "  - Compound: -0.999\n",
      "  --> Overall sentiment: Negative\n",
      "\n",
      "Analyzing sentiment of the first 5 sentences (as an example)...\n",
      "\n",
      "Sentence 1: 'Gangster tells BBC why India's biggest hip-hop star was murdered\n",
      "\n",
      "11 June 2025 Share Save Soutik Biswas & Ishleen Kaur BBC Eye Investigations Share Save\n",
      "\n",
      "BBC Sidhu Moose Wala was shot dead in a hail of bullets in 2022\n",
      "\n",
      "It was a killing that shocked India: Punjabi hip-hop star Sidhu Moose Wala shot dead through the windscreen of his car by hired gunmen.'\n",
      "  - Scores: {'neg': 0.241, 'neu': 0.611, 'pos': 0.148, 'compound': -0.891}\n",
      "\n",
      "Sentence 2: 'Within hours, a Punjabi gangster named Goldy Brar had used Facebook to claim responsibility for ordering the hit.'\n",
      "  - Scores: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\n",
      "Sentence 3: 'But three years after the murder, no-one has faced trial - and Goldy Brar is still on the run, his whereabouts unknown.'\n",
      "  - Scores: {'neg': 0.247, 'neu': 0.753, 'pos': 0.0, 'compound': -0.8201}\n",
      "\n",
      "Sentence 4: 'Now, BBC Eye has managed to make contact with Brar and challenged him about how and why Sidhu Moose Wala became a target.'\n",
      "  - Scores: {'neg': 0.062, 'neu': 0.938, 'pos': 0.0, 'compound': -0.1027}\n",
      "\n",
      "Sentence 5: 'His response was coldly articulate.'\n",
      "  - Scores: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Sentiment Analysis with NLTK VADER\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "print(\"\\n--- Performing Sentiment Analysis on the Article ---\")\n",
    "\n",
    "# We will use the 'raw_content' variable from the first cell, as VADER\n",
    "# works best with text that includes punctuation, capitalization, and stopwords.\n",
    "if 'raw_content' in locals() and raw_content and len(raw_content) > 1:\n",
    "    \n",
    "    # Download the VADER lexicon if not already present\n",
    "    try:\n",
    "        nltk.data.find('sentiment/vader_lexicon.zip')\n",
    "        print(\"VADER lexicon found.\")\n",
    "    except LookupError:\n",
    "        print(\"Downloading VADER lexicon...\")\n",
    "        # Use the same custom path to keep all NLTK data together\n",
    "        custom_nltk_data_path = \"C:\\\\nltk_data\"\n",
    "        nltk.download('vader_lexicon', download_dir=custom_nltk_data_path, quiet=False)\n",
    "\n",
    "    # Initialize the VADER sentiment analyzer\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # --- 1. Overall Article Sentiment ---\n",
    "    print(\"\\nAnalyzing sentiment of the entire article...\")\n",
    "    # Get the polarity scores for the whole raw text\n",
    "    overall_sentiment_scores = analyzer.polarity_scores(raw_content)\n",
    "    \n",
    "    print(\"\\nOverall Article Sentiment Scores:\")\n",
    "    print(f\"  - Negative: {overall_sentiment_scores['neg']:.3f}\")\n",
    "    print(f\"  - Neutral:  {overall_sentiment_scores['neu']:.3f}\")\n",
    "    print(f\"  - Positive: {overall_sentiment_scores['pos']:.3f}\")\n",
    "    print(f\"  - Compound: {overall_sentiment_scores['compound']:.3f}\")\n",
    "    \n",
    "    # Interpretation of the compound score\n",
    "    compound_score = overall_sentiment_scores['compound']\n",
    "    if compound_score >= 0.05:\n",
    "        print(\"  --> Overall sentiment: Positive\")\n",
    "    elif compound_score <= -0.05:\n",
    "        print(\"  --> Overall sentiment: Negative\")\n",
    "    else:\n",
    "        print(\"  --> Overall sentiment: Neutral\")\n",
    "\n",
    "\n",
    "    # --- 2. Sentence-level Sentiment (Example) ---\n",
    "    # This is useful for finding specific positive or negative statements.\n",
    "    print(\"\\nAnalyzing sentiment of the first 5 sentences (as an example)...\")\n",
    "    if 'sentences' in locals() and len(sentences) > 0:\n",
    "        for i, sentence in enumerate(sentences[:5]): # Analyze the first 5 sentences\n",
    "            sentence_scores = analyzer.polarity_scores(sentence)\n",
    "            print(f\"\\nSentence {i+1}: '{sentence}'\")\n",
    "            print(f\"  - Scores: {sentence_scores}\")\n",
    "            # You can add the Positive/Negative/Neutral interpretation here too if you want\n",
    "    else:\n",
    "        print(\"Could not find the 'sentences' list. Please ensure the BERTopic cell (Cell 2) has been run.\")\n",
    "\n",
    "else:\n",
    "    print(\"Variable 'raw_content' not found or is empty. Please run the previous cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4490cb1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
